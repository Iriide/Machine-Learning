# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z5YXx5kjAk3OvMyT2pjAc_rQP_nKkaZj
"""

import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
import pickle

"""1  Ładowanie danych"""

[test_set_raw, valid_set_raw, train_set_raw], info = tfds.load("tf_flowers", split=["train[:10%]", "train[10%:25%]", "train[25%:]"], as_supervised=True, with_info=True)

class_names = info.features["label"].names
n_classes = info.features["label"].num_classes
dataset_size = info.splits["train"].num_examples


plt.figure(figsize=(12,8))
index=0
sample_images=train_set_raw.take(9)
for image, label in sample_images:
  index+=1
  plt.subplot(3,3, index)
  plt.imshow(image)
  plt.title("Class:{}".format(class_names[label]))
  plt.axis("off")

plt.show(block=False)

"""2  Budujemy prostą sieć CNN"""

def preprocess(image, label):
  resized_image=tf.image.resize(image, [224,224])
  return resized_image, label

batch_size = 32
train_set = train_set_raw.map(preprocess).shuffle(dataset_size).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)

plt.figure(figsize=(8, 8))
sample_batch = train_set.take(1)
print(sample_batch)
for X_batch, y_batch in sample_batch:
    for index in range(12):
        plt.subplot(3, 4, index + 1)
        plt.imshow(X_batch[index]/255.0)
        plt.title("Class: {}".format(class_names[y_batch[index]]))
        plt.axis("off")
plt.show()

model = tf.keras.Sequential()

model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(224, 224, 3)))

model.add(layers.Conv2D(32, (7, 7), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(n_classes, activation='softmax'))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

model.fit(train_set, epochs=10, validation_data=valid_set)

test_loss, test_accuracy = model.evaluate(test_set)
train_loss, train_accuracy = model.evaluate(train_set)
val_loss, val_acc = model.evaluate(valid_set)

with open('simple_cnn_acc.pkl', 'wb') as f:
    pickle.dump((train_accuracy, val_acc, test_accuracy), f)

"""3  Uczenie transferowe"""

def preprocess(image, label):
    resized_image = tf.image.resize(image, [224, 224])
    final_image = tf.keras.applications.xception.preprocess_input(resized_image)
    return final_image, label

train_set = train_set_raw.map(preprocess).shuffle(dataset_size).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)


base_model = tf.keras.applications.xception.Xception(weights="imagenet", include_top=False)
x = base_model.output
x = tf.keras.layers.GlobalAveragePooling2D()(x)  # uśredniająca warstwa
outputs = tf.keras.layers.Dense(n_classes, activation='softmax')(x)  # gęsta warstwa wyjściowa

model = tf.keras.models.Model(inputs=base_model.input, outputs=outputs)

for layer in base_model.layers:
    layer.trainable = False

optimizer = tf.keras.optimizers.SGD(learning_rate=0.2, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.fit(train_set, validation_data=valid_set, epochs=1)


for layer in base_model.layers:
    layer.trainable = True
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.fit(train_set, validation_data=valid_set, epochs=1)

test_loss, test_accuracy = model.evaluate(test_set)
train_loss, train_accuracy = model.evaluate(train_set)
val_loss, val_acc = model.evaluate(valid_set)

with open('xception_acc.pkl', 'wb') as f:
    pickle.dump((train_accuracy, val_acc, test_accuracy), f)